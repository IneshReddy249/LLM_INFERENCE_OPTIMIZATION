{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "afNodXIx8gjR",
    "outputId": "1ec7c3bb-b736-434b-e743-630775df8523"
   },
   "outputs": [],
   "source": [
    "# Install core packages\n",
    "!pip install transformers accelerate torch datasets matplotlib pandas --quiet\n",
    "\n",
    "# Import libraries\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Check GPU and print its name\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"Total GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 194,
     "referenced_widgets": [
      "10f83a773ceb4f0dbca4899655557dad",
      "d08fa344f67940848bfafb62c7462677",
      "5fb372fc847b4a018e9baf8aa0bfbba9",
      "3a18a20d0815446da950565c96d9fa7d",
      "7908dbca3dd44040b536efb0490f94af",
      "ae061c39b21343e4969e4c3153356432",
      "80a9eaacd2a8444591c6b58b55a51bb1",
      "5fb5069a35c64b6bbe229365f15178bf",
      "a6062691b48d45d28f14683c1267734f",
      "e2025dcfc9194117a9c8ec07bb6486f8",
      "b2c09d188a334b0085d8e637f04dcaac"
     ]
    },
    "id": "0gIqA34t84wN",
    "outputId": "c28556a3-046e-4ba1-ef1c-9bafa0a14dca"
   },
   "outputs": [],
   "source": [
    "# Define the model name\n",
    "model_name = \"Qwen/Qwen-7B\"  # We'll use Qwen-7B as planned\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Load the model in FP16 precision to save memory\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,  # Use FP16 for efficiency\n",
    "    device_map=\"auto\",           # Automatically loads onto available GPUs\n",
    "    trust_remote_code=True,\n",
    "    use_cache=False # Explicitly disable caching as a troubleshooting step\n",
    ")\n",
    "\n",
    "print(\"Model and tokenizer loaded successfully in FP16 precision!\")\n",
    "print(f\"Model is on device: {model.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pyTxqRej9ew-",
    "outputId": "d1dd4b07-5351-4685-eda7-ee2b729a47db"
   },
   "outputs": [],
   "source": [
    "# Define our benchmarking function\n",
    "def benchmark_model(prompts, max_new_tokens=128):\n",
    "    \"\"\"\n",
    "    Runs inference on a list of prompts and returns key metrics.\n",
    "    \"\"\"\n",
    "    latencies = []\n",
    "    total_tokens_generated = 0\n",
    "\n",
    "    # Warm-up run\n",
    "    print(\"Performing warm-up run...\")\n",
    "    warm_up_input = tokenizer(\"Warm up\", return_tensors=\"pt\").to(\"cuda\")\n",
    "    # Add return_dict_in_generate=True to ensure the output is a dictionary\n",
    "    # Explicitly pass past_key_values=None and use_cache=False\n",
    "    _ = model.generate(**warm_up_input, max_new_tokens=2, return_dict_in_generate=True, past_key_values=None, use_cache=False)\n",
    "\n",
    "    # Clear GPU cache for accurate memory measurement\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    start_mem = torch.cuda.memory_allocated()\n",
    "\n",
    "    overall_start_time = time.time()\n",
    "\n",
    "    for prompt in prompts:\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "        # Time the generation\n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            # Add return_dict_in_generate=True for consistency\n",
    "            # Explicitly pass past_key_values=None and use_cache=False\n",
    "            outputs = model.generate(**inputs, max_new_tokens=max_new_tokens, return_dict_in_generate=True, past_key_values=None, use_cache=False)\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Calculate latency for this prompt\n",
    "        latency = end_time - start_time\n",
    "        latencies.append(latency)\n",
    "\n",
    "        # Calculate tokens generated (subtract input tokens)\n",
    "        # Access the generated sequences from the 'sequences' key\n",
    "        tokens_generated = len(outputs.sequences[0]) - len(inputs['input_ids'][0])\n",
    "        total_tokens_generated += tokens_generated\n",
    "\n",
    "    overall_end_time = time.time()\n",
    "    total_time = overall_end_time - overall_start_time\n",
    "\n",
    "    # Calculate memory usage\n",
    "    peak_mem = torch.cuda.max_memory_allocated()\n",
    "    memory_used = (peak_mem - start_mem) / 1e9  # Convert to GB\n",
    "\n",
    "    # Calculate metrics\n",
    "    avg_latency = sum(latencies) / len(latencies)\n",
    "    throughput = total_tokens_generated / total_time  # Tokens per second\n",
    "\n",
    "    results = {\n",
    "        \"avg_latency_sec\": avg_latency,\n",
    "        \"throughput_tok_sec\": throughput,\n",
    "        \"memory_used_gb\": memory_used,\n",
    "        \"total_tokens\": total_tokens_generated,\n",
    "        \"total_time_sec\": total_time\n",
    "    }\n",
    "    return results\n",
    "\n",
    "# Define your test prompts\n",
    "single_prompt = [\"Explain the concept of quantum computing in simple terms.\"]\n",
    "\n",
    "# For batch load, let's create multiple variations of a prompt\n",
    "batch_prompts = [\n",
    "    \"Explain the concept of quantum computing in simple terms.\",\n",
    "    \"Describe how a quantum computer works.\",\n",
    "    \"What is quantum supremacy?\",\n",
    "    \"How do qubits differ from classical bits?\",\n",
    "    \"What are the potential applications of quantum computing?\",\n",
    "    \"Explain quantum entanglement in simple terms.\",\n",
    "    \"What is a quantum algorithm?\",\n",
    "    \"How does Shor's algorithm work?\",\n",
    "    \"What are the challenges in building quantum computers?\",\n",
    "    \"Explain superposition in quantum computing.\",\n",
    "    \"What is quantum decoherence?\",\n",
    "    \"How do quantum gates work?\",\n",
    "    \"What is the difference between quantum and classical computing?\",\n",
    "    \"Explain quantum tunneling in computing.\",\n",
    "    \"What are quantum error correction techniques?\",\n",
    "    \"How does a quantum computer solve problems faster?\"\n",
    "]\n",
    "\n",
    "print(\"Benchmarking function and prompts defined successfully!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"RUNNING SINGLE PROMPT BENCHMARK\")\n",
    "print(\"=\"*50)\n",
    "single_results = benchmark_model(single_prompt)\n",
    "print(f\"Single Prompt Results:\")\n",
    "print(f\"- Average Latency: {single_results['avg_latency_sec']:.3f} sec\")\n",
    "print(f\"- Throughput: {single_results['throughput_tok_sec']:.2f} tokens/sec\")\n",
    "print(f\"- GPU Memory Used: {single_results['memory_used_gb']:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275
    },
    "id": "847fc1e5",
    "outputId": "e5903c9c-580a-4c4f-fff0-54237867c8fd"
   },
   "outputs": [],
   "source": [
    "# Prepare data for plotting\n",
    "labels = ['Single Prompt', 'Batch Prompts']\n",
    "avg_latencies = [single_results['avg_latency_sec'], batch_results['avg_latency_sec']]\n",
    "throughputs = [single_results['throughput_tok_sec'], batch_results['throughput_tok_sec']]\n",
    "memory_used = [single_results['memory_used_gb'], batch_results['memory_used_gb']]\n",
    "\n",
    "x = range(len(labels))\n",
    "\n",
    "# Create plots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot Average Latency\n",
    "axes[0].bar(x, avg_latencies, color=['skyblue', 'lightcoral'])\n",
    "axes[0].set_ylabel('Average Latency (sec)')\n",
    "axes[0].set_title('Average Latency Comparison')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(labels)\n",
    "axes[0].grid(axis='y', linestyle='--')\n",
    "\n",
    "# Plot Throughput\n",
    "axes[1].bar(x, throughputs, color=['skyblue', 'lightcoral'])\n",
    "axes[1].set_ylabel('Throughput (tokens/sec)')\n",
    "axes[1].set_title('Throughput Comparison')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(labels)\n",
    "axes[1].grid(axis='y', linestyle='--')\n",
    "\n",
    "# Plot GPU Memory Used\n",
    "axes[2].bar(x, memory_used, color=['skyblue', 'lightcoral'])\n",
    "axes[2].set_ylabel('GPU Memory Used (GB)')\n",
    "axes[2].set_title('GPU Memory Usage Comparison')\n",
    "axes[2].set_xticks(x)\n",
    "axes[2].set_xticklabels(labels)\n",
    "axes[2].grid(axis='y', linestyle='--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "61241c6f",
    "outputId": "217dd8a4-5d8c-4519-a7b3-5eddcf89648e"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"RUNNING BATCH PROMPT BENCHMARK\")\n",
    "print(\"=\"*50)\n",
    "batch_results = benchmark_model(batch_prompts)\n",
    "print(f\"Batch Prompt Results:\")\n",
    "print(f\"- Average Latency: {batch_results['avg_latency_sec']:.3f} sec\")\n",
    "print(f\"- Throughput: {batch_results['throughput_tok_sec']:.2f} tokens/sec\")\n",
    "print(f\"- GPU Memory Used: {batch_results['memory_used_gb']:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 305
    },
    "id": "l-CL0uOX-03r",
    "outputId": "435d536f-bf0a-4ced-d16c-b57c6aa4b707"
   },
   "outputs": [],
   "source": [
    "# Save results to a DataFrame\n",
    "results_df = pd.DataFrame({\n",
    "    'Scenario': ['Single', 'Batch'],\n",
    "    'Latency_sec': [single_results['avg_latency_sec'], batch_results['avg_latency_sec']],\n",
    "    'Throughput_tok_sec': [single_results['throughput_tok_sec'], batch_results['throughput_tok_sec']],\n",
    "    'Memory_GB': [single_results['memory_used_gb'], batch_results['memory_used_gb']]\n",
    "})\n",
    "\n",
    "# Save to CSV for later use\n",
    "results_df.to_csv('baseline_pytorch_results.csv', index=False)\n",
    "print(\"Baseline results saved to 'baseline_pytorch_results.csv'\")\n",
    "\n",
    "# Create visualizations\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "scenarios = ['Single Prompt', 'Batch Prompts']\n",
    "\n",
    "# Plot Latency\n",
    "axes[0].bar(scenarios, results_df['Latency_sec'])\n",
    "axes[0].set_title('Average Latency')\n",
    "axes[0].set_ylabel('Latency (seconds)')\n",
    "for i, v in enumerate(results_df['Latency_sec']):\n",
    "    axes[0].text(i, v, f'{v:.2f}', ha='center', va='bottom')\n",
    "\n",
    "# Plot Throughput\n",
    "axes[1].bar(scenarios, results_df['Throughput_tok_sec'])\n",
    "axes[1].set_title('Throughput')\n",
    "axes[1].set_ylabel('Tokens per Second')\n",
    "for i, v in enumerate(results_df['Throughput_tok_sec']):\n",
    "    axes[1].text(i, v, f'{v:.2f}', ha='center', va='bottom')\n",
    "\n",
    "# Plot Memory Usage\n",
    "axes[2].bar(scenarios, results_df['Memory_GB'])\n",
    "axes[2].set_title('GPU Memory Usage')\n",
    "axes[2].set_ylabel('Memory (GB)')\n",
    "for i, v in enumerate(results_df['Memory_GB']):\n",
    "    axes[2].text(i, v, f'{v:.2f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Baseline PyTorch Performance on A100 (Qwen-7B)', fontsize=16, y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "b933aa2e2070410a98c9caf9077a4166",
      "88a88650f84840febc0988e5218e5223",
      "10dcb6e8e3ec46fb81ca184b1c4306ef",
      "c0aaffe1b5e64485a0df6ad769c22a19",
      "0158125690ca4f64a334628844d2f7ea",
      "61d418235264405581d9fe0b72b92615",
      "f967e116212f4f608b337d87d33c6ffe",
      "f56f067d8134486a97bf429f38d1b2b2",
      "f47b6710041543a09488ee960b930419",
      "b9d38da82e994f67a3fc1902cf81a862",
      "fc406d3f92cf4c6a93ffdc44df4c2909"
     ]
    },
    "id": "V4_ADME_-8yZ",
    "outputId": "ba5914f3-faa9-499c-837a-1290bc9a9426"
   },
   "outputs": [],
   "source": [
    "# Install vLLM - this will give us the optimized inference engine\n",
    "!pip install vllm\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "import time\n",
    "\n",
    "# Initialize the optimized model with vLLM\n",
    "llm = LLM(\n",
    "    model=\"Qwen/Qwen-7B\",\n",
    "    dtype=\"half\",  # Use FP16 for fair comparison with our baseline\n",
    "    max_model_len=1024,  # Limit sequence length to save memory\n",
    "    trust_remote_code=True # Add this line to trust the remote code\n",
    ")\n",
    "\n",
    "# Define sampling parameters\n",
    "sampling_params = SamplingParams(temperature=0.0, max_tokens=128)\n",
    "\n",
    "print(\"vLLM optimized model loaded successfully!\")\n",
    "print(\"Key optimizations enabled: PagedAttention, Continuous Batching, FlashAttention\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qDdy5WzUAWdO",
    "outputId": "b6cac317-fcee-436f-a1c2-dc8a265d2504"
   },
   "outputs": [],
   "source": [
    "# Define benchmarking function for vLLM\n",
    "def benchmark_vllm(prompts, sampling_params):\n",
    "    \"\"\"\n",
    "    Runs inference using vLLM's optimized engine and returns key metrics.\n",
    "    \"\"\"\n",
    "    # Warm-up run\n",
    "    print(\"Performing warm-up run...\")\n",
    "    _ = llm.generate(\"Warm up\", sampling_params)\n",
    "\n",
    "    # Clear GPU cache for accurate memory measurement\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    start_mem = torch.cuda.memory_allocated()\n",
    "\n",
    "    overall_start_time = time.time()\n",
    "\n",
    "    # Run generation for all prompts\n",
    "    outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "    overall_end_time = time.time()\n",
    "    total_time = overall_end_time - overall_start_time\n",
    "\n",
    "    # Calculate memory usage\n",
    "    peak_mem = torch.cuda.max_memory_allocated()\n",
    "    memory_used = (peak_mem - start_mem) / 1e9  # Convert to GB\n",
    "\n",
    "    # Calculate total tokens generated\n",
    "    total_tokens_generated = 0\n",
    "    for output in outputs:\n",
    "        total_tokens_generated += len(output.outputs[0].token_ids)  # Count output tokens\n",
    "\n",
    "    # Calculate throughput\n",
    "    throughput = total_tokens_generated / total_time\n",
    "\n",
    "    # For latency, we calculate average time per prompt\n",
    "    avg_latency = total_time / len(prompts)\n",
    "\n",
    "    results = {\n",
    "        \"avg_latency_sec\": avg_latency,\n",
    "        \"throughput_tok_sec\": throughput,\n",
    "        \"memory_used_gb\": memory_used,\n",
    "        \"total_tokens\": total_tokens_generated,\n",
    "        \"total_time_sec\": total_time\n",
    "    }\n",
    "    return results\n",
    "\n",
    "print(\"vLLM benchmarking function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 594,
     "referenced_widgets": [
      "09ec204d52f2417fa4c8cc4f51b74f5d",
      "cd99de1babe64219b870f633bee9c6ea",
      "8524d4bd5fd24ee0a92ae991cb76d4a3",
      "6e8c0c9a89d74330a82ce4cf5e7dfbfa",
      "d45b4cbdbcb748208cc18353e87e0a3d",
      "3d5fa3ac1f2f4b1b918edf2418efa318",
      "f296284345d645cabc5f9cdf05dc0933",
      "d2afc533339f45639b35cf4a2d6531d1",
      "60dcc6fd9f264dd9a46dea4121d4a953",
      "94a932e51b9e4e42bfbd1dde81d5b3e4",
      "657485be737e4f91b9354051dccec4d1",
      "63c0031c5aff43018836cfd1158afae8",
      "ecb9f454fa794940bb718d5f203c1d39",
      "b6c0cebeec864a22a161167d8f326260",
      "64035daf35cd45459e42c849d63896fe",
      "94c925893dbf4628b73589104f879699",
      "697114c5fe684c08b2c8585e2dbd3877",
      "543b74a7d8df4e02b4933431dccd1ca6",
      "b5e9fa243eb94d11a8eba400c957c235",
      "1c205d66b6e1454f82a91a7da466d5e7",
      "b85d524e1bf2485c8694380ea7b78d9b",
      "14e0d94e3ecd486898b9b1c328958e8e",
      "7174bcc6615f42ec9f43b5a7830ca3ae",
      "96dade5a3d554914bb0126656ebb12cc",
      "181496b697f34066b179bcb29b53dc06",
      "85d760d416de4285a9800cfea21f5c91",
      "ede5c6e53bca448eb6daa7d543356444",
      "22254d9a23ec4b96b95c1f3b1a0a5b56",
      "07ba3ef61dec477fa0b5c14aaf2e7b70",
      "6f87606d63184050ad58088b5013c512",
      "9772d93482554d45bcea135593bc9829",
      "855b31ee5d144ebeb252cbcf11d646e0",
      "70ade49a110b4512a7f562ca9052103f",
      "40cd136619ad42f0b879a56926cf4ffd",
      "f8ec5b550e094064acdd74a38de1d878",
      "5ce76d5b54fb4628b0a1959e40840e28",
      "b4bf7193e16943febc76a8862366d99b",
      "c310ae69183741df8404b981ec2ac5b5",
      "a18cd5e14d2e4511bc656bfe35f20c4c",
      "eb4fa8f5e40d41ceb8c9781e988325a0",
      "156e884698284764ae77eeb2390d6b10",
      "fa759bf694e2497c93c425803a08d18f",
      "b1e4e908f753433faa07f439fcbf1a07",
      "00529cf1c24140938cad5287ad57ce93",
      "7c7625dde56a4e9b8f007f1f8be43060",
      "3a07c52dea424d61b7eade273776e04c",
      "759cc088c09a4fa091c8721c4912c392",
      "541bd3cd1c7241219e47730f345bc049",
      "34b6235cf32f4294ac555d55852e83de",
      "33977a1e62bb45aa83e2748c580c889c",
      "bb488c8d4c92426b859ca45da84f6ae7",
      "1df312c398364839bf95c693a686a265",
      "927e4bcbd8b74ee28839efafadab9667",
      "f2db84175d6846f4a61dd944e1217882",
      "b4758d3d600841f2b3dd9d4cc2a6eebc",
      "9ea8773de8a9411a9459f25037070452",
      "4591cfae4ac94439bcce75e0cb27bfd8",
      "63084bec3e2f4bfe9564ef43f6b84894",
      "63fb706330644246adacf37331db7a74",
      "fc84200320ad4e8ab21c5ef74ae2c79b",
      "d0b1c6d53e0044d3b8ecea086ddf9ce3",
      "969b33ac3eae4219a0a275d613f964c2",
      "0d4dc4d731954701a5b3f55a82a5e29e",
      "b68e081cd9bf40899ec9650e40781e81",
      "0e8e095fec9e456593a248545a8b6130",
      "7d0ddbcf638940c184d583972f340157",
      "ee68a67578184e3eb9445a1a3a426c65",
      "102053e89a7643f286d5d93b980c3f67",
      "cc402a83b38e4552bbf92da4deea04bc",
      "08996fbc27ba46d5a1a09fd34a382af0",
      "e2c9ead5e03d440dbe448867adf5060d",
      "b7931897b3a549509bcd0f056760ec51",
      "17b9f6ed1491487cbe6ab6895c0c1b40",
      "4943494851314cb18ce3ad43800fbf23",
      "2c6f7d9f872f41bda5903908748caf2b",
      "3739ae52dcbd4503b3769791c295625b",
      "e8f08ede646b4168aa7c197e112fb270",
      "b1cee59024b34426b439b1b0db4d2a1c",
      "430038f6b9274391912554e586c860bd",
      "2ffe3b17d50d4a1fa6c622c8aabf73cb",
      "185ff14855ae4847b01f15e16a723f3b",
      "0ef04267be2d47ee94373fb32ce85d33",
      "be5e0e62fa2140b3b675adf2304cf10d",
      "65b131f5ee764032b2cccfe3730acb72",
      "820a76dd3c9940bf80137bc3de7d1a6a",
      "00057c15b0e641f49410c5b52e8bbcf8",
      "b49955f0b07e45629077d2ab0d81b138",
      "75719abddd9943828224378e2dbde7b7"
     ]
    },
    "id": "wydQZtvGAhmX",
    "outputId": "bb0f6a77-7fb3-47da-9c3b-9720f1a66964"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "# Define your test prompts again within this cell\n",
    "single_prompt = [\"Explain the concept of quantum computing in simple terms.\"]\n",
    "\n",
    "# For batch load, let's create multiple variations of a prompt\n",
    "batch_prompts = [\n",
    "    \"Explain the concept of quantum computing in simple terms.\",\n",
    "    \"Describe how a quantum computer works.\",\n",
    "    \"What is quantum supremacy?\",\n",
    "    \"How do qubits differ from classical bits?\",\n",
    "    \"What are the potential applications of quantum computing?\",\n",
    "    \"Explain quantum entanglement in simple terms.\",\n",
    "    \"What is a quantum algorithm?\",\n",
    "    \"How does Shor's algorithm work?\",\n",
    "    \"What are the challenges in building quantum computers?\",\n",
    "    \"Explain superposition in quantum computing.\",\n",
    "    \"What is quantum decoherence?\",\n",
    "    \"How do quantum gates work?\",\n",
    "    \"What is the difference between quantum and classical computing?\",\n",
    "    \"Explain quantum tunneling in computing.\",\n",
    "    \"What are quantum error correction techniques?\",\n",
    "    \"How does a quantum computer solve problems faster?\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"RUNNING SINGLE PROMPT BENCHMARK (vLLM OPTIMIZED)\")\n",
    "print(\"=\"*50)\n",
    "vllm_single_results = benchmark_vllm(single_prompt, sampling_params)\n",
    "print(f\"vLLM Single Prompt Results:\")\n",
    "print(f\"- Average Latency: {vllm_single_results['avg_latency_sec']:.3f} sec\")\n",
    "print(f\"- Throughput: {vllm_single_results['throughput_tok_sec']:.2f} tokens/sec\")\n",
    "print(f\"- GPU Memory Used: {vllm_single_results['memory_used_gb']:.2f} GB\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"RUNNING BATCH PROMPT BENCHMARK (vLLM OPTIMIZED)\")\n",
    "print(\"=\"*50)\n",
    "vllm_batch_results = benchmark_vllm(batch_prompts, sampling_params)\n",
    "print(f\"vLLM Batch Prompt Results:\")\n",
    "print(f\"- Average Latency: {vllm_batch_results['avg_latency_sec']:.3f} sec\")\n",
    "print(f\"- Throughput: {vllm_batch_results['throughput_tok_sec']:.2f} tokens/sec\")\n",
    "print(f\"- GPU Memory Used: {vllm_batch_results['memory_used_gb']:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 462
    },
    "id": "37dab91f",
    "outputId": "50b6d606-0724-4ded-cccb-5af388416d1e"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load baseline results\n",
    "baseline_results_df = pd.read_csv('baseline_pytorch_results.csv')\n",
    "\n",
    "# Create DataFrame for vLLM results\n",
    "vllm_results_df = pd.DataFrame({\n",
    "    'Scenario': ['Single', 'Batch'],\n",
    "    'Latency_sec': [vllm_single_results['avg_latency_sec'], vllm_batch_results['avg_latency_sec']],\n",
    "    'Throughput_tok_sec': [vllm_single_results['throughput_tok_sec'], vllm_batch_results['throughput_tok_sec']],\n",
    "    'Memory_GB': [vllm_single_results['memory_used_gb'], vllm_batch_results['memory_used_gb']]\n",
    "})\n",
    "\n",
    "# Combine results for comparison\n",
    "comparison_df = pd.concat([baseline_results_df.assign(Implementation='PyTorch'),\n",
    "                           vllm_results_df.assign(Implementation='vLLM')])\n",
    "\n",
    "print(\"Comparison DataFrame created:\")\n",
    "display(comparison_df)\n",
    "\n",
    "# Create comparison plots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "bar_width = 0.35\n",
    "scenarios = ['Single', 'Batch']\n",
    "x = range(len(scenarios))\n",
    "\n",
    "# Plot Latency Comparison\n",
    "rects1 = axes[0].bar(x, comparison_df[comparison_df['Implementation'] == 'PyTorch']['Latency_sec'], bar_width, label='PyTorch')\n",
    "rects2 = axes[0].bar([p + bar_width for p in x], comparison_df[comparison_df['Implementation'] == 'vLLM']['Latency_sec'], bar_width, label='vLLM')\n",
    "axes[0].set_ylabel('Average Latency (sec)')\n",
    "axes[0].set_title('Average Latency Comparison')\n",
    "axes[0].set_xticks([p + bar_width / 2 for p in x])\n",
    "axes[0].set_xticklabels(scenarios)\n",
    "axes[0].legend()\n",
    "axes[0].grid(axis='y', linestyle='--')\n",
    "\n",
    "# Plot Throughput Comparison\n",
    "rects3 = axes[1].bar(x, comparison_df[comparison_df['Implementation'] == 'PyTorch']['Throughput_tok_sec'], bar_width, label='PyTorch')\n",
    "rects4 = axes[1].bar([p + bar_width for p in x], comparison_df[comparison_df['Implementation'] == 'vLLM']['Throughput_tok_sec'], bar_width, label='vLLM')\n",
    "axes[1].set_ylabel('Throughput (tokens/sec)')\n",
    "axes[1].set_title('Throughput Comparison')\n",
    "axes[1].set_xticks([p + bar_width / 2 for p in x])\n",
    "axes[1].set_xticklabels(scenarios)\n",
    "axes[1].legend()\n",
    "axes[1].grid(axis='y', linestyle='--')\n",
    "\n",
    "# Plot GPU Memory Used Comparison\n",
    "rects5 = axes[2].bar(x, comparison_df[comparison_df['Implementation'] == 'PyTorch']['Memory_GB'], bar_width, label='PyTorch')\n",
    "rects6 = axes[2].bar([p + bar_width for p in x], comparison_df[comparison_df['Implementation'] == 'vLLM']['Memory_GB'], bar_width, label='vLLM')\n",
    "axes[2].set_ylabel('GPU Memory Used (GB)')\n",
    "axes[2].set_title('GPU Memory Usage Comparison')\n",
    "axes[2].set_xticks([p + bar_width / 2 for p in x])\n",
    "axes[2].set_xticklabels(scenarios)\n",
    "axes[2].legend()\n",
    "axes[2].grid(axis='y', linestyle='--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('PyTorch vs vLLM Performance Comparison on A100 (Qwen-7B)', fontsize=16, y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 912
    },
    "id": "L7S94mszEpI5",
    "outputId": "91083aaf-eff5-4e17-f0ea-f4c3808b98eb"
   },
   "outputs": [],
   "source": [
    "# Let's create a complete comparison of all our results\n",
    "print(\"=\"*60)\n",
    "print(\"FINAL BENCHMARK RESULTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create a comparison table\n",
    "results_comparison = pd.DataFrame({\n",
    "    'Scenario': ['Single Prompt', 'Batch Prompts', 'Single Prompt', 'Batch Prompts'],\n",
    "    'Framework': ['PyTorch', 'PyTorch', 'vLLM', 'vLLM'],\n",
    "    'Latency (s)': [5.771, 5.044, 1.462, 0.098],\n",
    "    'Throughput (tokens/s)': [22.18, 22.76, 87.54, 1285.96],\n",
    "    'Memory (GB)': [0.05, 0.05, 0.00, 0.00]\n",
    "})\n",
    "\n",
    "print(results_comparison.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PERFORMANCE IMPROVEMENTS WITH VLLM\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculate improvement percentages\n",
    "single_latency_improvement = (5.771 - 1.462) / 5.771 * 100\n",
    "batch_latency_improvement = (5.044 - 0.098) / 5.044 * 100\n",
    "single_throughput_improvement = (87.54 - 22.18) / 22.18 * 100\n",
    "batch_throughput_improvement = (1285.96 - 22.76) / 22.76 * 100\n",
    "\n",
    "print(f\"Single Prompt Latency Improvement: {single_latency_improvement:.1f}%\")\n",
    "print(f\"Batch Prompts Latency Improvement: {batch_latency_improvement:.1f}%\")\n",
    "print(f\"Single Prompt Throughput Improvement: {single_throughput_improvement:.1f}%\")\n",
    "print(f\"Batch Prompts Throughput Improvement: {batch_throughput_improvement:.1f}%\")\n",
    "\n",
    "# Create final comparison visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Latency comparison\n",
    "frameworks = ['PyTorch', 'vLLM']\n",
    "single_latencies = [5.771, 1.462]\n",
    "batch_latencies = [5.044, 0.098]\n",
    "\n",
    "import numpy as np # Import numpy\n",
    "\n",
    "x = np.arange(len(frameworks))\n",
    "width = 0.35\n",
    "\n",
    "axes[0].bar(x - width/2, single_latencies, width, label='Single Prompt', color='blue')\n",
    "axes[0].bar(x + width/2, batch_latencies, width, label='Batch Prompts', color='orange')\n",
    "axes[0].set_title('Latency Comparison (Lower is Better)')\n",
    "axes[0].set_ylabel('Latency (seconds)')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(frameworks)\n",
    "axes[0].legend()\n",
    "axes[0].set_yscale('log')  # Log scale to show large differences\n",
    "\n",
    "# Throughput comparison\n",
    "single_throughputs = [22.18, 87.54]\n",
    "batch_throughputs = [22.76, 1285.96]\n",
    "\n",
    "axes[1].bar(x - width/2, single_throughputs, width, label='Single Prompt', color='blue')\n",
    "axes[1].bar(x + width/2, batch_throughputs, width, label='Batch Prompts', color='orange')\n",
    "axes[1].set_title('Throughput Comparison (Higher is Better)')\n",
    "axes[1].set_ylabel('Tokens per Second')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(frameworks)\n",
    "axes[1].legend()\n",
    "axes[1].set_yscale('log')  # Log scale to show large differences\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Comprehensive Performance Comparison: PyTorch vs vLLM on A100', fontsize=16, y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# Save final results\n",
    "results_comparison.to_csv('final_benchmark_results.csv', index=False)\n",
    "print(\"\\nFinal results saved to 'final_benchmark_results.csv'\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CONCLUSION\")\n",
    "print(\"=\"*60)\n",
    "print(\"vLLM provides significant performance improvements over standard PyTorch:\")\n",
    "print(\"- Up to 74.7% reduction in latency for single prompts\")\n",
    "print(f\"- Up to {batch_latency_improvement:.1f}% reduction in latency for batch processing\")\n",
    "print(f\"- Up to {single_throughput_improvement:.1f}% increase in throughput for single prompts\")\n",
    "print(f\"- Up to {batch_throughput_improvement:.1f}% increase in throughput for batch processing\")\n",
    "print(\"\\nThese improvements are achieved through:\")\n",
    "print(\"1. PagedAttention (efficient KV cache management)\")\n",
    "print(\"2. Continuous Batching (dynamic request handling)\")\n",
    "print(\"3. FlashAttention (optimized attention computation)\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
